{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* <b>PEFT technique:</b> LoRA \n",
    "* <b>Model</b>: GPT-2 \n",
    "* <b>Evaluation approach:</b> Accuracy, Precision, Recall, Specificity, NPV \n",
    "* <b>Fine-tuning dataset:</b> subset of carblacac/twitter-sentiment-analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c6bd8-469a-4d29-af48-e4610cd8766e",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "- <b>Link:</b> https://huggingface.co/datasets/imdb\n",
    "- <b>Task class:</b> Sentimental Analysis (Text Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978777a9-c429-4421-9b58-9c27246de18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kcymerys/Tools/python-envs/pythonML2/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for carblacac/twitter-sentiment-analysis contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/carblacac/twitter-sentiment-analysis\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['text', 'feeling'],\n",
       "     num_rows: 20000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'feeling'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'feeling'],\n",
       "     num_rows: 10000\n",
       " })]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"carblacac/twitter-sentiment-analysis\", split=['train[:20000]', 'validation[:1000]', 'test[:10000]'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af8c93e-8000-444f-a068-bb5bec1d4002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048bd24f-bd9a-486e-97cc-0f73da6051e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "train_dataset = dataset[0].map(tokenize_fn, batched=True)\n",
    "val_dataset = dataset[1].map(tokenize_fn, batched=True)\n",
    "test_dataset = dataset[2].map(tokenize_fn, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.add_column('label', train_dataset['feeling'])\n",
    "val_dataset = val_dataset.add_column('label', val_dataset['feeling'])\n",
    "test_dataset = test_dataset.add_column('label', test_dataset['feeling'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2',\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"}, \n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1})\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2a9d5b-4432-4097-a6ef-9ee78677a5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.250124216079712,\n",
       " 'eval_accuracy': 0.5057,\n",
       " 'eval_precision': 1.0,\n",
       " 'eval_recall': 0.5056505650565056,\n",
       " 'eval_negative_predictive_value': 0.0002022653721682848,\n",
       " 'eval_specificity': 1.0,\n",
       " 'eval_runtime': 67.7049,\n",
       " 'eval_samples_per_second': 147.7,\n",
       " 'eval_steps_per_second': 18.462}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # labels: 0 -> Negative 1 -> Positive\n",
    "    cls_correctness = (predictions == labels)\n",
    "    labels_bin = (labels == 1)\n",
    "    tp = ((cls_correctness) & (labels_bin)).sum()\n",
    "    tn = ((cls_correctness) & (~labels_bin)).sum()\n",
    "    fp = ((~cls_correctness) & (labels_bin)).sum()\n",
    "    fn = ((~cls_correctness) & (~labels_bin)).sum()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": (tp+tn)/(tp+tn+fp+fn),\n",
    "        \"precision\": tp/(tp+fp),\n",
    "        \"recall\": tp/(tp+fn),\n",
    "        \"negative_predictive_value\": tn/(tn+fn),\n",
    "        \"specificity\": tn/(tn+fp),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'gpt2', \n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"}, \n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    device_map='auto'\n",
    ")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbdafb5-de43-491b-9b07-9bc979bc2084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,252,352 || trainable%: 0.6474992182182735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kcymerys/Tools/python-envs/pythonML2/lib/python3.10/site-packages/peft/tuners/lora/layer.py:723: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): Linear(in_features=768, out_features=2, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, LoftQConfig, get_peft_model\n",
    "\n",
    "# https://github.com/TimDettmers/bitsandbytes/issues/762\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=['c_attn', 'c_proj'],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b0e262-d643-4ffb-9e26-3416c0f02876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6668' max='6668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6668/6668 48:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>0.414775</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>0.380856</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.420718</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6668, training_loss=0.3658030051704503, metrics={'train_runtime': 2886.9274, 'train_samples_per_second': 27.711, 'train_steps_per_second': 2.31, 'total_flos': 1.2537938140987392e+16, 'train_loss': 0.3658030051704503, 'epoch': 4.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./gpt2-peft\",\n",
    "        learning_rate=8e-4,\n",
    "        per_device_train_batch_size=12,\n",
    "        per_device_eval_batch_size=12,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        label_names=[\"labels\"]\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"gpt2-peft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2-peft\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"}, \n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    device_map='auto'\n",
    ")\n",
    "inference_model.config.pad_token_id = inference_model.config.eos_token_id\n",
    "inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be9f63f5-1161-4f07-a979-743c612e4312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4085777699947357,\n",
       " 'eval_accuracy': 0.8221,\n",
       " 'eval_precision': 0.8285205696202531,\n",
       " 'eval_recall': 0.8212115271515389,\n",
       " 'eval_negative_predictive_value': 0.8155339805825242,\n",
       " 'eval_specificity': 0.8230251071647275,\n",
       " 'eval_runtime': 73.7315,\n",
       " 'eval_samples_per_second': 135.627,\n",
       " 'eval_steps_per_second': 16.953}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # labels: 0 -> Negative 1 -> Positive\n",
    "    cls_correctness = (predictions == labels)\n",
    "    labels_bin = (labels == 1)\n",
    "    tp = ((cls_correctness) & (labels_bin)).sum()\n",
    "    tn = ((cls_correctness) & (~labels_bin)).sum()\n",
    "    fp = ((~cls_correctness) & (labels_bin)).sum()\n",
    "    fn = ((~cls_correctness) & (~labels_bin)).sum()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": (tp+tn)/(tp+tn+fp+fn),\n",
    "        \"precision\": tp/(tp+fp),\n",
    "        \"recall\": tp/(tp+fn),\n",
    "        \"negative_predictive_value\": tn/(tn+fn),\n",
    "        \"specificity\": tn/(tn+fp),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=inference_model,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c98f6b-c6e0-4b93-a51d-8c921b10bebb",
   "metadata": {},
   "source": [
    "## Comparision\n",
    "\n",
    "| Model\\Metric | Accuracy | Precision | Recall | Specificity | NPV |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Before PEFT | 0.51 | 1.0 | 0.51 | 1.0 | 0.00 |\n",
    "| After PEFT | 0.82 | 0.83 | 0.82 | 0.82 | 0.82 |\n",
    "\n",
    "It can be observed that: \n",
    "- PEFT procedure (LoRA) has allowed to improve classifier performance\n",
    "- Model before applying PEFT had tentency to classify samples as negative (Precision and \n",
    "\n",
    "Training on full dataset (not only subset as it can be biased towards one of classes) should allow to achieve even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ad74526-ef05-4224-b863-98f3f426a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: @justineville ...yeahhh. ) i'm 39 tweets from 1,600!\t Response: POSITIVE \t Known label: POSITIVE\n",
      "Sentence: @ApplesnFeathers aww. Poor baby! On your only REAL day off.\t Response: POSITIVE \t Known label: NEGATIVE\n",
      "Sentence: @joeymcintyre With my refunded $225 (Australian ticket price) I bought me a hot pair of brown boots  Woulda rathered seeing U any day\t Response: POSITIVE \t Known label: NEGATIVE\n",
      "Sentence: It's fine. Today sucks just because me those things. i dunno if i can see you\t Response: POSITIVE \t Known label: NEGATIVE\n",
      "Sentence: Im just chilling on psp and stuff, but sitting on pc now, also watching wimledon, getting ready for holiday @WhiteTigerNora Ahh poor you\t Response: POSITIVE \t Known label: NEGATIVE\n",
      "Sentence: @lisarinna very sad Lisa...she is freeeeeeeeeeee an Angel in Heaven xoxo\t Response: POSITIVE \t Known label: NEGATIVE\n",
      "Sentence: Comfortablity has won out\t Response: POSITIVE \t Known label: NEGATIVE\n",
      "Sentence: blaaah. I don't feel good aagain\t Response: POSITIVE \t Known label: NEGATIVE\n",
      "Sentence: @cly_lit Get 100 followers a day using www.tweeteradder.com Once you add everyone you are on the train or pay vip\t Response: POSITIVE \t Known label: POSITIVE\n",
      "Sentence: My oldest son is making a trip with my mother... IÃ¯Â¿Â½m missing him\t Response: POSITIVE \t Known label: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def predict(sentence: str, known_label: str) -> None:\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(inference_model.device)\n",
    "        logits = inference_model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        predicted_class_id = probabilities.argmax().item()\n",
    "        output = inference_model.config.id2label[predicted_class_id]\n",
    "        print(f'Sentence: {sentence}\\t Response: {output} \\t Known label: {inference_model.config.id2label[known_label]}')\n",
    "        \n",
    "for test_sentence, known_label in zip(test_dataset[:10]['text'], test_dataset[:10]['label']):\n",
    "    predict(test_sentence, known_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9faf7f-575b-4f0a-8249-e2115846cac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newPythonML2",
   "language": "python",
   "name": "newpythonml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
